from __future__ import annotations
import os, json, logging, argparse, sys
from datetime import datetime, timedelta
from typing import List, Dict, Optional

from apify_client import ApifyClient
from dotenv import load_dotenv

load_dotenv()

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(name)s | %(message)s",
    datefmt="%H:%M:%S",
)
log = logging.getLogger("FbCrawlerApify")


class FacebookCrawlerApify:
    """Thu th·∫≠p b√†i vi·∫øt qua Apify Actor API v√† tr·∫£ v·ªÅ c√°c tr∆∞·ªùng r√∫t g·ªçn."""

    DEFAULT_POST_ACTOR = "apify/facebook-posts-scraper"

    def __init__(self) -> None:
        self.token: str | None = os.getenv("APIFY_TOKEN")
        if not self.token:
            raise ValueError("‚ö†  APIFY_TOKEN ph·∫£i khai b√°o trong .env ho·∫∑c ENV")
        self.page_id: str | None = os.getenv("FB_PAGE_ID")
        if not self.page_id:
            raise ValueError("‚ö†  FB_PAGE_ID ph·∫£i khai b√°o trong .env ho·∫∑c ENV")

        self.post_actor = os.getenv("APIFY_POST_ACTOR", self.DEFAULT_POST_ACTOR)
        self.client = ApifyClient(self.token)

    # --------------------------------------------------
    def fetch_posts(self, *, days: int = 10, limit: Optional[int] = None) -> List[Dict]:
        """Tr·∫£ v·ªÅ **nh·ªØng b√†i vi·∫øt** trong `days` ng√†y g·∫ßn nh·∫•t.
        - `limit` = None ‚Üí l·∫•y t·∫•t c·∫£.
        """
        since = (datetime.utcnow() - timedelta(days=days)).strftime("%Y-%m-%d")
        log.info("‚ñ∂Ô∏è  Call actor %s cho page %s", self.post_actor, self.page_id)

        results_limit = limit * 5 if limit else 10000  # ƒë·∫£m b·∫£o kh√¥ng c·∫Øt s·ªõm
        run_input = {
            "startUrls": [{"url": f"https://www.facebook.com/{self.page_id}"}],
            "resultsLimit": results_limit,
            "onlyPostsNewerThan": since,
            "proxyConfig": {"useApifyProxy": True},
        }
        run = self.client.actor(self.post_actor).call(run_input=run_input)
        raw_items = list(self.client.dataset(run["defaultDatasetId"]).iterate_items())
        log.info("üìÑ  Nh·∫≠n %d b√†i (ch∆∞a l·ªçc)", len(raw_items))

        posts: List[Dict] = []
        for it in raw_items:
            # --------------- ∆Øu ti√™n l·∫•y ·∫£nh t·ª´ photo_image.uri -------------
            images: List[str] = []
            media = it.get("media")
            if isinstance(media, list):
                for m in media:
                    if not isinstance(m, dict):
                        continue
                    uri: Optional[str] = None
                    # ∆∞u ti√™n tr∆∞·ªùng photo_image.uri
                    if isinstance(m.get("photo_image"), dict):
                        uri = m["photo_image"].get("uri")
                    # fallback sang url n·∫øu uri kh√¥ng c√≥
                    if not uri:
                        uri = m.get("url")
                    if uri and uri.startswith("http"):
                        images.append(uri)
            # lo·∫°i tr√πng
            images = list(dict.fromkeys(images))

            # --------------- L·∫•y s·ªë like, comment ---------------------------
            likes_cnt = (
                it.get("likes")
                or it.get("likesCount")
                or it.get("topReactionsCount")
                or it.get("reactions")
                or 0
            )
            comments_cnt = (
                it.get("comments")
                or it.get("commentsCount")
                or it.get("commentCount")
                or 0
            )

            post = {
                "text": it.get("text", ""),
                "images": images,
                "url": it.get("url") or it.get("topLevelUrl"),
                "likes": int(likes_cnt),
                "comments": int(comments_cnt),
            }
            posts.append(post)
            if limit and len(posts) >= limit:
                break

        log.info("‚úÖ  Ho√†n t·∫•t ‚Äì tr·∫£ v·ªÅ %d b√†i", len(posts))
        return posts



# ---------------- CLI ---------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Facebook Crawler via Apify API (fields r√∫t g·ªçn)")
    parser.add_argument("--days", type=int, default=10, help="S·ªë ng√†y g·∫ßn nh·∫•t")
    group = parser.add_mutually_exclusive_group()
    group.add_argument("--limit", type=int, help="S·ªë b√†i t·ªëi ƒëa (m·∫∑c ƒë·ªãnh: t·∫•t c·∫£)")
    group.add_argument("--all", action="store_true", help="L·∫•y t·∫•t c·∫£ b√†i")
    args = parser.parse_args(sys.argv[1:])

    lim = None if args.all or args.limit is None else args.limit
    crawler = FacebookCrawlerApify()
    data = crawler.fetch_posts(days=args.days, limit=lim)
    print(json.dumps(data, ensure_ascii=False, indent=2))
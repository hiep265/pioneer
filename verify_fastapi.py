import os, json, logging, random, re, argparse, sys, asyncio, hashlib
from datetime import datetime, timedelta
from typing import List, Dict

from bs4 import BeautifulSoup
from pyppeteer import launch
from pyppeteer.page import Page
from pyppeteer.browser import Browser
from dotenv import load_dotenv

load_dotenv()

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(name)s | %(message)s",
    datefmt="%H:%M:%S",
)
log = logging.getLogger("FbCrawlerPuppeteer")


class FacebookCrawler:
    MOBILE_UA = (
        "Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) "
        "AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Mobile/15E148 Safari/604.1"
    )

    # ------------ kh·ªüi t·∫°o ------------------------------------------------
    @classmethod
    async def create(cls, *, desktop: bool = False):
        self = cls()
        self.desktop = desktop

        self.page_id = os.getenv("FB_PAGE_ID")
        if not self.page_id:
            raise ValueError("‚ö†  FB_PAGE_ID ph·∫£i khai b√°o trong .env")

        txt_path = "cookies/www.facebook.com_cookies.txt"
        json_path = "cookies/fb_cookie.json"
        self.cookie_path = (
            txt_path
            if os.path.isfile(txt_path)
            else json_path
            if os.path.isfile(json_path)
            else os.getenv("FB_COOKIES", json_path)
        )
        if not os.path.isfile(self.cookie_path):
            raise FileNotFoundError(f"Cookie file '{self.cookie_path}' kh√¥ng t·ªìn t·∫°i")

        launch_options = {
            "headless": False,
            "args": [
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",
                "--disable-gpu",
                "--lang=vi-VN",
                "--start-maximized",
            ],
            "ignoreHTTPSErrors": True,
            # üëâ ch·ªânh ƒë∆∞·ªùng Chrome ph√π h·ª£p m√°y b·∫°n
            "executablePath": r"C:\Program Files\Google\Chrome\Application\chrome.exe",
        }

        log.info("[OPEN] M·ªü Chrome giao di·ªán th·∫≠t ‚Ä¶")
        self.browser: Browser = await launch(**launch_options)
        self.page: Page = (await self.browser.pages())[0]

        if self.desktop:
            await self.page.setUserAgent(
                self.MOBILE_UA.replace(
                    "iPhone; CPU iPhone OS 15_0 like Mac OS X",
                    "Windows NT 10.0; Win64; x64",
                )
            )
            await self.page.setViewport({"width": 1920, "height": 1080})
        else:
            await self.page.setUserAgent(self.MOBILE_UA)
            await self.page.setViewport({"width": 800, "height": 1200})

        await self._inject_cookies()
        return self

    async def close(self):
        if self.browser:
            await self.browser.close()

    # ------------ utils ---------------------------------------------------
    async def _expand_see_more(self):
        """B·∫•m t·∫•t c·∫£ n√∫t/nh√£n 'Xem th√™m' hi·ªán trong viewport."""
        js = """
        () => {
          const btns = Array.from(document.querySelectorAll('span, a, div'));
          btns.filter(el => {
             const t = (el.innerText || '').trim();
             return t === 'Xem th√™m' || t === 'See more';
          }).forEach(el => el.click());
        }
        """
        try:
            await self.page.evaluate(js)
            await asyncio.sleep(0.8)
        except Exception as e:
            log.debug(f"Kh√¥ng click ƒë∆∞·ª£c See more: {e}")

    # ------------ crawl ---------------------------------------------------
    async def fetch_posts(
    self,
    days: int = 10,
    limit: int = 10,
    max_scroll: int = 120,      # du di n·∫øu feed d√†i
    idle_retry: int = 4         # s·ªë v√≤ng cu·ªôn ‚Äútr∆°‚Äù li√™n ti·∫øp th√¨ d·ª´ng
) -> List[Dict]:
        log.info("Scraping %s (‚â§%s ng√†y, c·∫ßn %s b√†i)", self.page_id, days, limit)

        base = "www.facebook.com" if self.desktop else "m.facebook.com"
        await self.page.goto(f"https://{base}/{self.page_id}", {"waitUntil": "networkidle2"})
        await asyncio.sleep(300)

        posts, seen_ids = [], set()
        scrolls, idle_cnt = 0, 0
        last_height, last_art_cnt = 0, 0

        while scrolls < max_scroll and len(posts) < limit:
            # 1Ô∏è‚É£ m·ªü caption d√†i trong viewport
            await self._expand_see_more()

            # 2Ô∏è‚É£ l·∫•y danh s√°ch article
            html  = await self.page.content()
            soup  = BeautifulSoup(html, "lxml")
            arts  = self._get_articles(soup)
            log.info("üí¨ X√©t %d article (ƒë√£ c√≥ %d h·ª£p l·ªá)", len(arts), len(posts))

            added = 0
            for art in arts:
                if len(posts) >= limit:
                    break
                post = self._parse_article(art)
                if not post:
                    continue
                pid = str(post["post_id"]).strip()
                if pid in seen_ids or post["text"].strip().lower() == "ho quoc tuan":
                    continue
                posts.append(post)
                seen_ids.add(pid)
                added += 1

            # 3Ô∏è‚É£ ki·ªÉm tra c√≥ ti·∫øn tri·ªÉn?
            curr_height   = await self.page.evaluate("document.body.scrollHeight")
            curr_art_cnt  = len(arts)
            progressed    = (added > 0) or (curr_height > last_height) or (curr_art_cnt > last_art_cnt)

            if progressed:
                idle_cnt = 0
            else:
                idle_cnt += 1
                if idle_cnt >= idle_retry:
                    log.info("üòï Cu·ªôn %s l·∫ßn li·ªÅn kh√¥ng n·∫°p b√†i m·ªõi ‚Äì d·ª´ng.", idle_retry)
                    break

            last_height, last_art_cnt = curr_height, curr_art_cnt

            # 4Ô∏è‚É£ cu·ªôn s√°t ƒë√°y ƒë·ªÉ FB auto-load
            await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await asyncio.sleep(random.uniform(1.2, 2.0))
            scrolls += 1

        log.info("‚úÖ Thu ƒë∆∞·ª£c %d/%d b√†i h·ª£p l·ªá sau %d l·∫ßn cu·ªôn", len(posts), limit, scrolls)
        return posts[:limit]

    # ------------ helpers -------------------------------------------------
    def _get_articles(self, soup: BeautifulSoup) -> List[BeautifulSoup]:
        # ch·ªâ c√°c container feed c√≥ data-tracking-duration-id
        arts = soup.select(
            "div.m.bg-s2[data-mcomponent='MContainer'][data-type='container'][data-tracking-duration-id]"
        )

        good = []
        for art in arts:
            cap_tag = art.select_one("div.native-text.rslh")
            has_caption = bool(cap_tag)
            has_media = art.select_one("img[data-type='image'], video") is not None

            # b·ªè n·∫øu kh√¥ng c√≥ caption & media
            if not (has_caption or has_media):
                continue

            # caption ng·∫Øn ho·∫∑c ch·ª©a t·ª´ kho√° giao di·ªán => b·ªè
            if has_caption:
                txt = cap_tag.get_text(strip=True)
                bad_kw = ("ng∆∞·ªùi theo d√µi", "B√†i vi·∫øt", "Gi·ªõi thi·ªáu", "Chi ti·∫øt")
                if len(txt) <= 10 or any(k.lower() in txt.lower() for k in bad_kw):
                    continue

            good.append(art)

        return good


    def _fallback_post_id(self, art, caption: str) -> str:
        """N·∫øu kh√¥ng t√¨m th·∫•y id qua link ‚Üí l·∫•y `data-image-id` ‚Üí hash caption."""
        img_tag = art.select_one("img[data-image-id]")
        if img_tag and img_tag.get("data-image-id"):
            return img_tag["data-image-id"]
        # hash caption (·ªïn ƒë·ªãnh cho c√πng caption)
        return hashlib.md5(caption.encode("utf-8")).hexdigest()

    def _parse_article(self, art) -> Dict | None:
        # 1. caption
        cap_tag = art.select_one("div.native-text.rslh")
        caption = cap_tag.get_text(" ", strip=True) if cap_tag else ""

        # 2. ·∫£nh
        img_tags = art.select("img[data-type='image']")
        imgs = [img["src"] for img in img_tags if "https://" in img.get("src", "")]

        # 3. n·∫øu kh√¥ng c√≥ n·ªôi dung g√¨ th√¨ b·ªè qua
        if not caption and not imgs:
            return None

        # 4. l·ªçc block v·ªõ v·∫©n (header/avatar)
        bad_kw = ("ng∆∞·ªùi theo d√µi", "B√†i vi·∫øt", "Gi·ªõi thi·ªáu", "Chi ti·∫øt")
        if any(k.lower() in caption.lower() for k in bad_kw):
            return None

        # 5. l·∫•y post_id
        post_id = None
        for tag in img_tags:
            pid = tag.get("data-image-id")
            if pid:
                post_id = pid
                break
        if not post_id:
            post_id = self._fallback_post_id(art, caption)

        if caption.strip().lower() == "ho quoc tuan":
            return None

        # 6. comment (tu·ª≥ ch·ªçn)
        cmt_tags = art.select("div[dir='auto'][data-visualcompletion='ignore-dynamic']")[:2]
        comments = [c.get_text(" ", strip=True) for c in cmt_tags]

        return {
            "post_id": post_id,
            "text": caption,
            "url": None,
            "images": imgs,
            "comments": comments,
        }


    # ------------ cookies -------------------------------------------------
    async def _inject_cookies(self):
        cookies_to_set = []
        if self.cookie_path.endswith(".json"):
            with open(self.cookie_path, encoding="utf-8") as fp:
                cookies = json.load(fp)
            for c in cookies:
                cookies_to_set.append(
                    {
                        "name": c.get("name"),
                        "value": c.get("value"),
                        "domain": c.get("domain", ".facebook.com"),
                        "path": c.get("path", "/"),
                        "expires": c.get("expirationDate", -1),
                        "httpOnly": c.get("httpOnly", False),
                        "secure": c.get("secure", False),
                    }
                )
        else:
            with open(self.cookie_path, encoding="utf-8") as fp:
                for line in fp:
                    if not line.startswith(".facebook.com"):
                        continue
                    parts = line.strip().split("\t")
                    if len(parts) == 7:
                        cookies_to_set.append(
                            {
                                "name": parts[5],
                                "value": parts[6],
                                "domain": parts[0],
                                "path": parts[2],
                                "expires": int(parts[4]),
                                "secure": parts[3] == "TRUE",
                            }
                        )

        await self.page.setCookie(*cookies_to_set)
        log.info("ƒê√£ n·∫°p %d cookie", len(cookies_to_set))


# ---------------- CLI -----------------------------------------------------
async def main():
    parser = argparse.ArgumentParser(description="Facebook Crawler (Pyppeteer)")
    parser.add_argument("--desktop", action="store_true", help="D√πng giao di·ªán desktop")
    parser.add_argument("--days", type=int, default=10, help="S·ªë ng√†y g·∫ßn nh·∫•t")
    parser.add_argument("--limit", type=int, default=10, help="S·ªë b√†i t·ªëi ƒëa")
    args = parser.parse_args(sys.argv[1:])

    crawler = None
    try:
        crawler = await FacebookCrawler.create(desktop=args.desktop)
        data = await crawler.fetch_posts(days=args.days, limit=args.limit)
        print(json.dumps(data, ensure_ascii=False, indent=2))
    except Exception as e:
        log.error(f"L·ªói: {e}", exc_info=True)
    finally:
        if crawler:
            await crawler.close()


if __name__ == "__main__":
    asyncio.run(main())
